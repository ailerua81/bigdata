{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d9bfe3",
   "metadata": {},
   "source": [
    "# Big Data Analytics — Assignment 03\n",
    "> Author : Badr TAJINI - Big Data Analytics - ESIEE 2025-2026\n",
    "\n",
    "**Chapter 5 :** Graphs (PageRank/PPR)   \n",
    "**Chapter 6 :** Spam classification (SGD) in PySpark\n",
    "\n",
    "**Tools :** Spark or PySpark.   \n",
    "**Advice:** Keep evidence and reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ca3c4",
   "metadata": {},
   "source": [
    "## Setup global d'initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8d458e-6210-4ef3-a747-d014e1f5668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete!\n",
      "  BASE_DIR: /home/aurel/bda_labs/bda_assignment03\n",
      "  DATA_DIR: /home/aurel/bda_labs/bda_assignment03/data\n",
      "  OUTPUTS_DIR: /home/aurel/bda_labs/bda_assignment03/outputs\n"
     ]
    }
   ],
   "source": [
    "# imports et chemins globaux\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import bz2\n",
    "import shutil\n",
    "import gzip\n",
    "import math\n",
    "import random\n",
    "from operator import add\n",
    "from contextlib import redirect_stdout\n",
    "from io import StringIO\n",
    "\n",
    "# Définir les chemins globaux\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUTS_DIR = BASE_DIR / \"outputs\"\n",
    "PROOF_DIR = BASE_DIR / \"proof\"\n",
    "\n",
    "# Créer les dossiers\n",
    "for directory in (DATA_DIR, OUTPUTS_DIR, PROOF_DIR):\n",
    "    directory.mkdir(exist_ok=True)\n",
    "\n",
    "# Définir les chemins des fichiers\n",
    "spam_dir = DATA_DIR / \"spam\"\n",
    "gnutella_path = DATA_DIR / \"p2p-Gnutella08-adj.txt\"\n",
    "spam_train_britney_path = DATA_DIR / \"spam.train.britney.txt\"\n",
    "spam_train_group_x_path = DATA_DIR / \"spam.train.group_x.txt\"\n",
    "spam_train_group_y_path = DATA_DIR / \"spam.train.group_y.txt\"\n",
    "spam_qrels_path = DATA_DIR / \"spam.test.qrels.txt\"\n",
    "\n",
    "print(\"✓ Setup complete!\")\n",
    "print(f\"  BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"  DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"  OUTPUTS_DIR: {OUTPUTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5756c0-fdf2-4a70-bd23-f46d26dfdc5f",
   "metadata": {},
   "source": [
    "## 0. Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "206af0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 13:53:08 WARN Utils: Your hostname, PCPORTABLEAUR resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/06 13:53:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/06 13:53:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark initialized!\n",
      "  Spark version: 3.5.0\n",
      "  PySpark version: 3.5.0\n",
      "  Python version: 3.10.19\n"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - create SparkSession('BDA-A03') with UTC timezone\n",
    "# - print Spark/PySpark/Python versions\n",
    "# - set spark.sql.shuffle.partitions for local runs\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "# Arrêter Spark s'il existe déjà\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped existing Spark session\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Créer une nouvelle session avec paramètres optimisés\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"BDA-A03\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")  # Réduit pour éviter surcharge\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.default.parallelism\", \"2\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"✓ Spark initialized!\")\n",
    "print(f\"  Spark version: {spark.version}\")\n",
    "print(f\"  PySpark version: {pyspark.__version__}\")\n",
    "print(f\"  Python version: {sys.version.split()[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f5c57",
   "metadata": {},
   "source": [
    "## 1. Dataset acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a644ac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Graphe existant trouvé: ~6301 nœuds\n",
      "  Fichier: /home/aurel/bda_labs/bda_assignment03/data/p2p-Gnutella08-adj.txt\n",
      "Décompression des fichiers spam...\n",
      "  ✓ spam.test.qrels.txt déjà décompressé\n",
      "  ✓ spam.train.britney.txt déjà décompressé\n",
      "  ✓ spam.train.group_x.txt déjà décompressé\n",
      "  ✓ spam.train.group_y.txt déjà décompressé\n",
      "\n",
      "✓ Acquisition des données terminée!\n"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - ensure data/p2p-Gnutella08-adj.txt exists (convert from SNAP edgelist if needed)\n",
    "# - ensure spam.train.* and spam.test.qrels.txt exist (download + bunzip2)\n",
    "# - quick sanity checks on file sizes and line counts\n",
    "\n",
    "if not gnutella_path.exists():\n",
    "    print(\"Graphe non trouvé. Création d'un graphe synthétique...\")\n",
    "    print(\"(Pour utiliser le vrai graphe Gnutella, téléchargez-le manuellement)\")\n",
    "    \n",
    "    # Créer un graphe synthétique de taille raisonnable\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    \n",
    "    num_nodes = 200  # Taille modérée pour éviter les crashs\n",
    "    adjacency = {}\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        node = str(i)\n",
    "        adjacency[node] = []\n",
    "        \n",
    "        # Chaque nœud a 3-8 voisins\n",
    "        num_neighbors = random.randint(3, 8)\n",
    "        for _ in range(num_neighbors):\n",
    "            # Préférence pour les nœuds bas (simulation scale-free)\n",
    "            if random.random() < 0.6 and i > 20:\n",
    "                neighbor = str(random.randint(0, min(30, i-1)))\n",
    "            else:\n",
    "                neighbor = str(random.randint(0, num_nodes-1))\n",
    "            \n",
    "            if neighbor != node and neighbor not in adjacency[node]:\n",
    "                adjacency[node].append(neighbor)\n",
    "    \n",
    "    with open(gnutella_path, 'w') as f:\n",
    "        for node, neighbors in sorted(adjacency.items()):\n",
    "            if neighbors:\n",
    "                f.write(f\"{node} {' '.join(neighbors)}\\n\")\n",
    "    \n",
    "    print(f\"✓ Graphe synthétique créé: {num_nodes} nœuds\")\n",
    "    print(f\"  Fichier: {gnutella_path}\")\n",
    "else:\n",
    "    # Compter les nœuds\n",
    "    num_nodes = sum(1 for _ in open(gnutella_path))\n",
    "    print(f\"✓ Graphe existant trouvé: ~{num_nodes} nœuds\")\n",
    "    print(f\"  Fichier: {gnutella_path}\")\n",
    "\n",
    "\n",
    "spam_files = [\n",
    "    (\"spam.test.qrels.txt.bz2\", spam_qrels_path),\n",
    "    (\"spam.train.britney.txt.bz2\", spam_train_britney_path),\n",
    "    (\"spam.train.group_x.txt.bz2\", spam_train_group_x_path),\n",
    "    (\"spam.train.group_y.txt.bz2\", spam_train_group_y_path),\n",
    "]\n",
    "\n",
    "print(\"Décompression des fichiers spam...\")\n",
    "for bz2_filename, txt_path in spam_files:\n",
    "    if not txt_path.exists():\n",
    "        bz2_path = spam_dir / bz2_filename\n",
    "        if bz2_path.exists():\n",
    "            print(f\"  Décompression de {bz2_filename}...\")\n",
    "            with bz2.open(bz2_path, 'rb') as src, open(txt_path, 'wb') as dst:\n",
    "                shutil.copyfileobj(src, dst)\n",
    "            print(f\"    ✓ {txt_path.name}\")\n",
    "        else:\n",
    "            print(f\"    ✗ {bz2_filename} non trouvé dans {spam_dir}\")\n",
    "    else:\n",
    "        print(f\"  ✓ {txt_path.name} déjà décompressé\")\n",
    "\n",
    "print(\"\\n✓ Acquisition des données terminée!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22d97a",
   "metadata": {},
   "source": [
    "## 2. Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7fdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - parse adjacency-list line 'u v1 v2 ...' to (u, [v1, v2, ...])\n",
    "# - utility for top-k without collect: use takeOrdered on (rank, node) with key\n",
    "# - formatting helpers to save top-20 CSVs\n",
    "\n",
    "def parse_adjacency_line(line):\n",
    "    \"\"\"Parse 'u v1 v2 ...' to (u, [v1, v2, ...])\"\"\"\n",
    "    parts = line.strip().split()\n",
    "    if not parts:\n",
    "        return None\n",
    "    node = parts[0]\n",
    "    neighbors = parts[1:] if len(parts) > 1 else []\n",
    "    return (node, neighbors)\n",
    "\n",
    "def format_topk_csv(topk_list, output_path):\n",
    "    \"\"\"Save top-k list to CSV\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(\"node,score\\n\")\n",
    "        for node, score in topk_list:\n",
    "            f.write(f\"{node},{score:.10f}\\n\")\n",
    "    print(f\"✓ Saved top-{len(topk_list)} to {output_path}\")\n",
    "\n",
    "def parse_spam_line(line):\n",
    "    \"\"\"Parse spam line: 'docid label f1 f2 ...' (binary features)\"\"\"\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    \n",
    "    docid = parts[0]\n",
    "    label_str = parts[1].lower()\n",
    "    label = 1.0 if label_str == 'spam' else 0.0\n",
    "    \n",
    "    features = {}\n",
    "    for token in parts[2:]:\n",
    "        try:\n",
    "            if ':' in token:\n",
    "                feat_id, value = token.split(':', 1)\n",
    "                features[int(feat_id)] = float(value)\n",
    "            else:\n",
    "                feat_id = int(token)\n",
    "                features[feat_id] = 1.0\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "    \n",
    "    return (docid, label, features)\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d3369",
   "metadata": {},
   "source": [
    "## 3. Part A — PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3537321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PageRank ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 6301\n",
      "Running PageRank (alpha=0.85, 10 iterations)...\n",
      "  Iteration 02 | mass=1.000000\n",
      "  Iteration 04 | mass=1.000000\n",
      "  Iteration 06 | mass=1.000000\n",
      "  Iteration 08 | mass=1.000000\n",
      "  Iteration 10 | mass=1.000000\n",
      "✓ Saved top-20 to /home/aurel/bda_labs/bda_assignment03/outputs/pagerank_top20.csv\n",
      "✓ PageRank complete! Top node: ('367', 0.002387885575274488)\n"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - parameters: alpha=0.85, iterations, partitions\n",
    "# - initialize ranks uniformly; build adjacency RDD partitioned by key\n",
    "# - iterative loop: contributions + missing mass redistribution\n",
    "# - compute top-20 without collect; write outputs/pagerank_top20.csv\n",
    "# - save any DF stage plan to proof/plan_pr.txt\n",
    "\n",
    "\n",
    "print(\"\\n=== PageRank ===\")\n",
    "\n",
    "alpha = 0.85\n",
    "num_iters = 10\n",
    "k = 20\n",
    "\n",
    "# Charger le graphe\n",
    "lines_rdd = spark.sparkContext.textFile(str(gnutella_path))\n",
    "adjacency_rdd = (\n",
    "    lines_rdd\n",
    "    .map(parse_adjacency_line)\n",
    "    .filter(lambda x: x is not None)\n",
    "    .partitionBy(2)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "nodes_rdd = adjacency_rdd.keys().cache()\n",
    "num_nodes = nodes_rdd.count()\n",
    "print(f\"Total nodes: {num_nodes}\")\n",
    "\n",
    "# Initialiser les rangs uniformément\n",
    "ranks = nodes_rdd.map(lambda node: (node, 1.0 / num_nodes))\n",
    "\n",
    "print(f\"Running PageRank (alpha={alpha}, {num_iters} iterations)...\")\n",
    "\n",
    "for iteration in range(1, num_iters + 1):\n",
    "    joined = adjacency_rdd.join(ranks)\n",
    "    \n",
    "    dangling_mass = (\n",
    "        joined\n",
    "        .filter(lambda kv: len(kv[1][0]) == 0)\n",
    "        .map(lambda kv: kv[1][1])\n",
    "        .sum()\n",
    "    )\n",
    "    \n",
    "    contribs = (\n",
    "        joined\n",
    "        .flatMap(lambda kv: \n",
    "            [] if len(kv[1][0]) == 0 \n",
    "            else [(nbr, kv[1][1] / len(kv[1][0])) for nbr in kv[1][0]]\n",
    "        )\n",
    "        .reduceByKey(add)\n",
    "    )\n",
    "    \n",
    "    teleport_mass = (1.0 - alpha) + alpha * dangling_mass\n",
    "    base = (\n",
    "        nodes_rdd\n",
    "        .map(lambda node: (node, 0.0))\n",
    "        .leftOuterJoin(contribs)\n",
    "        .mapValues(lambda pair: pair[1] if pair[1] is not None else 0.0)\n",
    "    )\n",
    "    \n",
    "    ranks = base.map(lambda kv: (kv[0], alpha * kv[1] + teleport_mass / num_nodes))\n",
    "    \n",
    "    total_mass = ranks.values().sum()\n",
    "    ranks = ranks.mapValues(lambda v: v / total_mass)\n",
    "    \n",
    "    if iteration % 2 == 0:\n",
    "        preview = ranks.takeOrdered(3, key=lambda kv: -kv[1])\n",
    "        print(f\"  Iteration {iteration:02d} | mass={total_mass:.6f}\")\n",
    "\n",
    "# Top-K\n",
    "pr_topk = ranks.takeOrdered(k, key=lambda kv: -kv[1])\n",
    "pr_output_path = OUTPUTS_DIR / \"pagerank_top20.csv\"\n",
    "format_topk_csv(pr_topk, pr_output_path)\n",
    "\n",
    "# Sauvegarder le plan d'exécution\n",
    "from pyspark.sql import functions as F\n",
    "pr_df = spark.createDataFrame(pr_topk, schema=[\"node\", \"score\"]).orderBy(F.desc(\"score\"))\n",
    "plan_buffer = StringIO()\n",
    "with redirect_stdout(plan_buffer):\n",
    "    pr_df.explain(\"formatted\")\n",
    "(PROOF_DIR / \"plan_pr.txt\").write_text(plan_buffer.getvalue())\n",
    "\n",
    "print(f\"✓ PageRank complete! Top node: {pr_topk[0]}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f4d0ab",
   "metadata": {},
   "source": [
    "## 4. Part A — Multi-Source Personalized PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fafcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Personalized PageRank ===\n",
      "Sources: ['367', '249', '145']\n",
      "  Iteration 02 | mass=1.000000\n",
      "  Iteration 04 | mass=1.000000\n",
      "  Iteration 06 | mass=1.000000\n",
      "  Iteration 08 | mass=1.000000\n",
      "  Iteration 10 | mass=1.000000\n",
      "✓ Saved top-20 to /home/aurel/bda_labs/bda_assignment03/outputs/ppr_top20.csv\n",
      "✓ PPR complete! Top node: ('367', 0.13299374907556302)\n"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - parameters: sources list, alpha, iterations, partitions\n",
    "# - init mass 1/|S| on sources; others 0\n",
    "# - on jump and dangling mass, teleport uniformly to S\n",
    "# - use mapPartitions(..., preservesPartitioning=True) when transforming keyed RDDs\n",
    "# - compute top-20 and write outputs/ppr_top20.csv\n",
    "# - save any DF stage plan to proof/plan_ppr.txt\n",
    "\n",
    "\n",
    "print(\"\\n=== Personalized PageRank ===\")\n",
    "\n",
    "# Utiliser les 3 meilleurs nœuds comme sources\n",
    "sources = [node for node, _ in pr_topk[:3]]\n",
    "source_set = set(sources)\n",
    "initial_mass = 1.0 / len(source_set)\n",
    "\n",
    "print(f\"Sources: {sources}\")\n",
    "\n",
    "ppr_ranks = nodes_rdd.map(lambda node: (node, initial_mass if node in source_set else 0.0))\n",
    "\n",
    "for iteration in range(1, num_iters + 1):\n",
    "    joined = adjacency_rdd.join(ppr_ranks)\n",
    "    \n",
    "    dangling_mass = (\n",
    "        joined\n",
    "        .filter(lambda kv: len(kv[1][0]) == 0)\n",
    "        .map(lambda kv: kv[1][1])\n",
    "        .sum()\n",
    "    )\n",
    "    \n",
    "    contribs = (\n",
    "        joined\n",
    "        .flatMap(lambda kv: \n",
    "            [] if len(kv[1][0]) == 0 \n",
    "            else [(nbr, kv[1][1] / len(kv[1][0])) for nbr in kv[1][0]]\n",
    "        )\n",
    "        .reduceByKey(add)\n",
    "    )\n",
    "    \n",
    "    teleport_mass = (1.0 - alpha) + alpha * dangling_mass\n",
    "    jump_mass = teleport_mass / len(source_set)\n",
    "    \n",
    "    base = (\n",
    "        nodes_rdd\n",
    "        .map(lambda node: (node, 0.0))\n",
    "        .leftOuterJoin(contribs)\n",
    "        .mapValues(lambda pair: pair[1] if pair[1] is not None else 0.0)\n",
    "    )\n",
    "    \n",
    "    ppr_ranks = base.map(lambda kv: (\n",
    "        kv[0], \n",
    "        alpha * kv[1] + (jump_mass if kv[0] in source_set else 0.0)\n",
    "    ))\n",
    "    \n",
    "    total_mass = ppr_ranks.values().sum()\n",
    "    ppr_ranks = ppr_ranks.mapValues(lambda v: v / total_mass)\n",
    "    \n",
    "    if iteration % 2 == 0:\n",
    "        print(f\"  Iteration {iteration:02d} | mass={total_mass:.6f}\")\n",
    "\n",
    "ppr_topk = ppr_ranks.takeOrdered(k, key=lambda kv: -kv[1])\n",
    "ppr_output_path = OUTPUTS_DIR / \"ppr_top20.csv\"\n",
    "format_topk_csv(ppr_topk, ppr_output_path)\n",
    "\n",
    "ppr_df = spark.createDataFrame(ppr_topk, schema=[\"node\", \"score\"]).orderBy(F.desc(\"score\"))\n",
    "plan_buffer = StringIO()\n",
    "with redirect_stdout(plan_buffer):\n",
    "    ppr_df.explain(\"formatted\")\n",
    "(PROOF_DIR / \"plan_ppr.txt\").write_text(plan_buffer.getvalue())\n",
    "\n",
    "print(f\"✓ PPR complete! Top node: {ppr_topk[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378f6a9",
   "metadata": {},
   "source": [
    "## 5. Part B — TrainSpamClassifier (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22417a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part B: Train Spam Classifier ===\n",
      "Loading training data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spam_train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m train_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mspam_train_path\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     48\u001b[0m         parsed \u001b[38;5;241m=\u001b[39m parse_spam_line(line)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spam_train_path' is not defined"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - parameters: delta, epochs, shuffle flag, numReducers=1\n",
    "# - read training lines: docid label f1 f2 ...\n",
    "# - emit (0, (docid, isSpam, features)) and groupByKey(1) to a single learner\n",
    "# - implement SGD updates on the reducer side; save model to outputs/model_*/part-00000\n",
    "\n",
    "\n",
    "print(\"\\n=== Train Spam Classifier ===\")\n",
    "\n",
    "# Vérifier que le fichier existe\n",
    "if not spam_train_britney_path.exists():\n",
    "    print(f\"✗ ERROR: {spam_train_britney_path} not found!\")\n",
    "    print(\"  Run CELLULE 3 first to decompress data.\")\n",
    "else:\n",
    "    # Charger les données\n",
    "    print(\"Loading training data...\")\n",
    "    train_data = []\n",
    "    with open(spam_train_britney_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parsed = parse_spam_line(line)\n",
    "            if parsed:\n",
    "                docid, label, features = parsed\n",
    "                train_data.append((label, features))\n",
    "    \n",
    "    print(f\"✓ Loaded {len(train_data)} samples\")\n",
    "    \n",
    "    # Debug première ligne\n",
    "    if len(train_data) > 0:\n",
    "        sample_label, sample_features = train_data[0]\n",
    "        print(f\"  First sample: label={sample_label}, features={len(sample_features)}\")\n",
    "    \n",
    "    # Entraîner SGD\n",
    "    print(\"\\nTraining SGD classifier...\")\n",
    "    weights = {}\n",
    "    bias = 0.0\n",
    "    learning_rate = 0.1\n",
    "    reg = 1e-5\n",
    "    epochs = 5\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1.0 / (1.0 + math.exp(-max(-500, min(500, x))))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        for label, features in train_data:\n",
    "            dot = bias\n",
    "            for idx, value in features.items():\n",
    "                dot += weights.get(idx, 0.0) * value\n",
    "            \n",
    "            pred = sigmoid(dot)\n",
    "            error = pred - label\n",
    "            \n",
    "            for idx, value in features.items():\n",
    "                w = weights.get(idx, 0.0)\n",
    "                grad = error * value + reg * w\n",
    "                weights[idx] = w - learning_rate * grad\n",
    "            \n",
    "            bias -= learning_rate * (error + reg * bias)\n",
    "        \n",
    "        learning_rate *= 0.9\n",
    "        print(f\"  Epoch {epoch + 1}/{epochs} | weights={len(weights)} | bias={bias:.4f}\")\n",
    "    \n",
    "    print(f\"\\n✓ Training complete!\")\n",
    "    print(f\"  Model size: {len(weights)} features\")\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    model_output_dir = OUTPUTS_DIR / \"model_spam\"\n",
    "    model_output_dir.mkdir(exist_ok=True)\n",
    "    model_output_path = model_output_dir / \"part-00000\"\n",
    "    \n",
    "    with open(model_output_path, 'w') as f:\n",
    "        for feat_id, weight in sorted(weights.items()):\n",
    "            f.write(f\"{feat_id}\\t{weight}\\n\")\n",
    "        f.write(f\"-1\\t{bias}\\n\")\n",
    "    \n",
    "    print(f\"✓ Model saved to {model_output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500620d9",
   "metadata": {},
   "source": [
    "#### 6. Part B — ApplySpamClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a8f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Part B: Apply Spam Classifier ===\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/aurel/bda_labs/bda_assignment03/outputs/model_spam/part-00000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_output_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m         parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bda-env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/aurel/bda_labs/bda_assignment03/outputs/model_spam/part-00000'"
     ]
    }
   ],
   "source": [
    "# write some code here\n",
    "# - load model tuple file to dict or broadcast\n",
    "# - score test instances and emit (docid, score, predicted_label)\n",
    "# - write outputs/predictions_*/\n",
    "\n",
    "print(\"\\n=== Apply Spam Classifier ===\")\n",
    "\n",
    "# Charger le modèle\n",
    "model_output_path = OUTPUTS_DIR / \"model_spam\" / \"part-00000\"\n",
    "if not model_output_path.exists():\n",
    "    print(\"✗ ERROR: Model not found!\")\n",
    "    print(f\"  Expected path: {model_output_path}\")\n",
    "    print(f\"  Please run CELLULE 7 first to train the model.\")\n",
    "    print(\"\\nTo verify:\")\n",
    "    print(f\"  1. Check if CELLULE 7 executed successfully\")\n",
    "    print(f\"  2. Check if folder exists: {OUTPUTS_DIR / 'model_spam'}\")\n",
    "    raise FileNotFoundError(f\"Model not found at {model_output_path}\")\n",
    "\n",
    "# Si on arrive ici, le modèle existe\n",
    "model = {}\n",
    "with open(model_output_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                feat_id = int(parts[0])\n",
    "                weight = float(parts[1])\n",
    "                model[feat_id] = weight\n",
    "    \n",
    "bias = model.get(-1, 0.0)\n",
    "print(f\"✓ Model loaded: {len(model)-1} features, bias={bias:.4f}\")\n",
    "\n",
    "\n",
    "# Split les données d'entraînement en train/test\n",
    "random.seed(42)\n",
    "random.shuffle(train_data)\n",
    "split_idx = int(0.8 * len(train_data))\n",
    "test_data = train_data[split_idx:]\n",
    "        \n",
    "print(f\"Test set: {len(test_data)} samples\")\n",
    "        \n",
    "# Prédire\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + math.exp(-max(-500, min(500, x))))\n",
    "    \n",
    "predictions = []\n",
    "for label, features in test_data:\n",
    "    dot = bias\n",
    "    for idx, value in features.items():\n",
    "        dot += model.get(idx, 0.0) * value\n",
    "        \n",
    "        score = sigmoid(dot)\n",
    "        pred = 1.0 if score >= 0.5 else 0.0\n",
    "        predictions.append((label, score, pred))\n",
    "    \n",
    "    # Évaluer\n",
    "    tp = fp = fn = tn = 0\n",
    "    for true_label, score, pred in predictions:\n",
    "        if true_label == 1.0 and pred == 1.0:\n",
    "            tp += 1\n",
    "        elif true_label == 0.0 and pred == 1.0:\n",
    "            fp += 1\n",
    "        elif true_label == 1.0 and pred == 0.0:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "    \n",
    "    # Calculer AUC\n",
    "    sorted_scores = sorted(predictions, key=lambda x: x[1])\n",
    "    pos = sum(1 for label, _, _ in sorted_scores if label == 1.0)\n",
    "    neg = len(sorted_scores) - pos\n",
    "    rank_sum = sum(rank for rank, (label, _, _) in enumerate(sorted_scores, 1) if label == 1.0)\n",
    "    auc = (rank_sum - pos * (pos + 1) / 2.0) / (pos * neg) if pos and neg else 0.0\n",
    "    \n",
    "    print(f\"\\n✓ Evaluation Results:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  AUC:       {auc:.4f}\")\n",
    "    \n",
    "    # Sauvegarder les métriques\n",
    "    metrics_lines = [\n",
    "        \"# Spam Classification Metrics\",\n",
    "        \"\",\n",
    "        f\"- Precision: {precision:.4f}\",\n",
    "        f\"- Recall: {recall:.4f}\",\n",
    "        f\"- F1-Score: {f1:.4f}\",\n",
    "        f\"- AUC: {auc:.4f}\",\n",
    "        \"\",\n",
    "        \"## Confusion Matrix\",\n",
    "        f\"- TP: {tp}, FP: {fp}\",\n",
    "        f\"- FN: {fn}, TN: {tn}\",\n",
    "    ]\n",
    "    \n",
    "    metrics_path = OUTPUTS_DIR / \"metrics.md\"\n",
    "    metrics_path.write_text('\\n'.join(metrics_lines))\n",
    "    print(f\"✓ Metrics saved to {metrics_path}\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d876a",
   "metadata": {},
   "source": [
    "## 7. Part B — ApplyEnsembleSpamClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code here\n",
    "# - --method average or vote\n",
    "# - load multiple part-00000 model files; broadcast\n",
    "# - average scores or majority vote; write outputs and a small sample\n",
    "\n",
    "print(\"\\n=== Part B: Ensemble Classifier ===\")\n",
    "\n",
    "# Train models on all three datasets\n",
    "ensemble_models = {}\n",
    "ensemble_train_files = [\n",
    "    (\"britney\", spam_train_britney_path),\n",
    "    (\"group_x\", spam_train_group_x_path),\n",
    "    (\"group_y\", spam_train_group_y_path),\n",
    "]\n",
    "\n",
    "print(\"Training ensemble models...\")\n",
    "for name, train_file_path in ensemble_train_files:\n",
    "    print(f\"\\n  Training on {name}...\")\n",
    "    \n",
    "    # Load and prepare training data\n",
    "    train_for_ensemble = (\n",
    "        spark.sparkContext.textFile(str(train_file_path))\n",
    "        .map(parse_spam_line)\n",
    "        .filter(lambda x: x is not None)\n",
    "    )\n",
    "    \n",
    "    # Train using SGD\n",
    "    keyed_train_ensemble = train_for_ensemble.map(lambda x: (0, x))\n",
    "    model_rdd_ensemble = keyed_train_ensemble.groupByKey(numPartitions=1).mapPartitions(sgd_train)\n",
    "    \n",
    "    # Store model\n",
    "    ensemble_models[name] = dict(model_rdd_ensemble.collect())\n",
    "    print(f\"    ✓ Model {name}: {len(ensemble_models[name])} features\")\n",
    "\n",
    "# Save ensemble models\n",
    "for name, model_dict in ensemble_models.items():\n",
    "    model_dir = OUTPUTS_DIR / f\"model_ensemble_{name}\"\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    model_path = model_dir / \"part-00000\"\n",
    "    \n",
    "    with open(model_path, 'w') as f:\n",
    "        for feat_id, weight in sorted(model_dict.items()):\n",
    "            f.write(f\"{feat_id}\\t{weight}\\n\")\n",
    "    \n",
    "    print(f\"Saved ensemble model {name} to {model_path}\")\n",
    "\n",
    "# Apply ensemble - Method 1: Average scores\n",
    "print(\"\\n=== Ensemble Method: Average Scores ===\")\n",
    "\n",
    "def score_with_ensemble(docid, label, features, method='average'):\n",
    "    \"\"\"Score with ensemble of models\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for name, model_dict in ensemble_models.items():\n",
    "        bias_m = model_dict.get(-1, 0.0)\n",
    "        dot = bias_m\n",
    "        for feat_id, value in features.items():\n",
    "            dot += model_dict.get(feat_id, 0.0) * value\n",
    "        score = sigmoid(dot)\n",
    "        scores.append(score)\n",
    "    \n",
    "    if method == 'average':\n",
    "        final_score = sum(scores) / len(scores)\n",
    "    elif method == 'vote':\n",
    "        # Majority vote\n",
    "        votes = [1.0 if s >= 0.5 else 0.0 for s in scores]\n",
    "        final_score = sum(votes) / len(votes)\n",
    "        final_score = 1.0 if final_score > 0.5 else 0.0\n",
    "    else:\n",
    "        final_score = scores[0]  # fallback\n",
    "    \n",
    "    predicted = 1.0 if final_score >= 0.5 else 0.0\n",
    "    return (docid, final_score, predicted, label)\n",
    "\n",
    "# Score test data with ensemble\n",
    "ensemble_predictions = test_data.map(lambda x: score_with_ensemble(x[0], x[1], x[2], method='average'))\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_preds_list = ensemble_predictions.collect()\n",
    "tp_ens = fp_ens = fn_ens = tn_ens = 0\n",
    "\n",
    "for docid, score, pred, true_label in ensemble_preds_list:\n",
    "    if true_label == 1.0 and pred == 1.0:\n",
    "        tp_ens += 1\n",
    "    elif true_label == 0.0 and pred == 1.0:\n",
    "        fp_ens += 1\n",
    "    elif true_label == 1.0 and pred == 0.0:\n",
    "        fn_ens += 1\n",
    "    else:\n",
    "        tn_ens += 1\n",
    "\n",
    "precision_ens = tp_ens / (tp_ens + fp_ens) if (tp_ens + fp_ens) else 0.0\n",
    "recall_ens = tp_ens / (tp_ens + fn_ens) if (tp_ens + fn_ens) else 0.0\n",
    "f1_ens = 2 * precision_ens * recall_ens / (precision_ens + recall_ens) if (precision_ens + recall_ens) else 0.0\n",
    "\n",
    "# Compute AUC for ensemble\n",
    "scored_ens = [(score, true_label) for docid, score, pred, true_label in ensemble_preds_list]\n",
    "sorted_ens = sorted(scored_ens, key=lambda x: x[0])\n",
    "pos_ens = sum(1 for _, label in sorted_ens if label == 1.0)\n",
    "neg_ens = len(sorted_ens) - pos_ens\n",
    "rank_sum_ens = sum(rank for rank, (score, label) in enumerate(sorted_ens, start=1) if label == 1.0)\n",
    "auc_ens = (rank_sum_ens - pos_ens * (pos_ens + 1) / 2.0) / (pos_ens * neg_ens) if pos_ens and neg_ens else 0.0\n",
    "\n",
    "# Save ensemble predictions\n",
    "ensemble_output_dir = OUTPUTS_DIR / \"predictions_ensemble_average\"\n",
    "if ensemble_output_dir.exists():\n",
    "    shutil.rmtree(ensemble_output_dir)\n",
    "ensemble_output_dir.mkdir(exist_ok=True)\n",
    "ensemble_predictions.saveAsTextFile(str(ensemble_output_dir))\n",
    "\n",
    "print(f\"\\nEnsemble Performance (Average Method):\")\n",
    "print(f\"  Precision: {precision_ens:.4f}\")\n",
    "print(f\"  Recall: {recall_ens:.4f}\")\n",
    "print(f\"  F1-Score: {f1_ens:.4f}\")\n",
    "print(f\"  AUC: {auc_ens:.4f}\")\n",
    "\n",
    "# Append to metrics file\n",
    "with open(metrics_path, 'a') as f:\n",
    "    f.write(\"\\n\\n## Ensemble Model Performance (Average)\\n\")\n",
    "    f.write(f\"- Precision: {precision_ens:.4f}\\n\")\n",
    "    f.write(f\"- Recall: {recall_ens:.4f}\\n\")\n",
    "    f.write(f\"- F1-Score: {f1_ens:.4f}\\n\")\n",
    "    f.write(f\"- AUC: {auc_ens:.4f}\\n\")\n",
    "    f.write(f\"- Models: britney, group_x, group_y\\n\")\n",
    "\n",
    "# Optional: Vote method\n",
    "print(\"\\n=== Ensemble Method: Majority Vote ===\")\n",
    "ensemble_vote_predictions = test_data.map(lambda x: score_with_ensemble(x[0], x[1], x[2], method='vote'))\n",
    "ensemble_vote_dir = OUTPUTS_DIR / \"predictions_ensemble_vote\"\n",
    "if ensemble_vote_dir.exists():\n",
    "    shutil.rmtree(ensemble_vote_dir)\n",
    "ensemble_vote_dir.mkdir(exist_ok=True)\n",
    "ensemble_vote_predictions.saveAsTextFile(str(ensemble_vote_dir))\n",
    "print(f\"Vote predictions saved to {ensemble_vote_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a7dd7",
   "metadata": {},
   "source": [
    "## 8. Evaluation and shuffle study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code here\n",
    "# - compute ROC-AUC with Spark ML if desired\n",
    "# - or invoke external compute_spam_metrics if available (optional)\n",
    "# - implement --shuffle: random key + sortBy to permute training before SGD\n",
    "# - run 10 trials on britney; summarize in outputs/metrics.md\n",
    "\n",
    "print(\"\\n=== Part B: Evaluation ===\")\n",
    "\n",
    "# Evaluate predictions directly from test split\n",
    "predictions_list = predictions_rdd.collect()\n",
    "tp = fp = fn = tn = 0\n",
    "\n",
    "for docid, score, pred, true_label in predictions_list:\n",
    "    if true_label == 1.0 and pred == 1.0:\n",
    "        tp += 1\n",
    "    elif true_label == 0.0 and pred == 1.0:\n",
    "        fp += 1\n",
    "    elif true_label == 1.0 and pred == 0.0:\n",
    "        fn += 1\n",
    "    else:\n",
    "        tn += 1\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "# Compute AUC\n",
    "scored_with_labels = [(score, true_label) for docid, score, pred, true_label in predictions_list]\n",
    "sorted_scores = sorted(scored_with_labels, key=lambda x: x[0])\n",
    "\n",
    "pos = sum(1 for _, label in sorted_scores if label == 1.0)\n",
    "neg = len(sorted_scores) - pos\n",
    "\n",
    "rank_sum = 0.0\n",
    "for rank, (score, label) in enumerate(sorted_scores, start=1):\n",
    "    if label == 1.0:\n",
    "        rank_sum += rank\n",
    "\n",
    "auc = (rank_sum - pos * (pos + 1) / 2.0) / (pos * neg) if pos and neg else 0.0\n",
    "\n",
    "metrics_lines = [\n",
    "    \"# Spam Classification Metrics\",\n",
    "    \"\",\n",
    "    f\"## Model Performance\",\n",
    "    f\"- Precision: {precision:.4f}\",\n",
    "    f\"- Recall: {recall:.4f}\",\n",
    "    f\"- F1-Score: {f1:.4f}\",\n",
    "    f\"- AUC: {auc:.4f}\",\n",
    "    \"\",\n",
    "    f\"## Confusion Matrix\",\n",
    "    f\"- True Positives: {tp}\",\n",
    "    f\"- False Positives: {fp}\",\n",
    "    f\"- False Negatives: {fn}\",\n",
    "    f\"- True Negatives: {tn}\",\n",
    "    \"\",\n",
    "    f\"## Training Parameters\",\n",
    "    f\"- Epochs: {epochs}\",\n",
    "    f\"- Learning rate: 0.1 (initial), 0.9 decay per epoch\",\n",
    "    f\"- Regularization: 1e-5\",\n",
    "    f\"- Training file: spam.train.britney.txt\",\n",
    "    \"\",\n",
    "    f\"## Dataset Split\",\n",
    "    f\"- Train: 80%\",\n",
    "    f\"- Test: 20%\",\n",
    "]\n",
    "\n",
    "metrics_path = OUTPUTS_DIR / \"metrics.md\"\n",
    "metrics_path.write_text('\\n'.join(metrics_lines))\n",
    "print('\\n'.join(metrics_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479520c0",
   "metadata": {},
   "source": [
    "## 9. Spark UI evidence\n",
    "Open http://localhost:4040 during runs. Capture Files Read, Input Size, Shuffle Read/Write for representative stages; store under `proof/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83751e",
   "metadata": {},
   "source": [
    "## 10. Environment and reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb62573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code here\n",
    "# - print Java version, Spark conf of interest, OS info\n",
    "# - save ENV.md with versions + key configs\n",
    "\n",
    "\n",
    "print(\"\\n=== Environment Summary ===\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def get_java_version():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"java\", \"-version\"], stderr=subprocess.STDOUT)\n",
    "        return output.decode(\"utf-8\").strip().splitlines()[0]\n",
    "    except Exception as exc:\n",
    "        return f\"Unavailable ({exc})\"\n",
    "\n",
    "java_version = get_java_version()\n",
    "print(f\"Java: {java_version}\")\n",
    "\n",
    "conf_items = sorted(spark.sparkContext.getConf().getAll())\n",
    "\n",
    "env_lines = [\n",
    "    \"# Environment Summary\",\n",
    "    \"\",\n",
    "    f\"- Python: {sys.version.split()[0]}\",\n",
    "    f\"- Spark: {spark.version}\",\n",
    "    f\"- PySpark: {pyspark.__version__}\",\n",
    "    f\"- Java: {java_version}\",\n",
    "    f\"- OS: {platform.platform()}\",\n",
    "    \"\",\n",
    "    \"## Spark Configuration\",\n",
    "]\n",
    "\n",
    "env_lines.extend(f\"- {k} = {v}\" for k, v in conf_items)\n",
    "\n",
    "env_path = BASE_DIR / \"ENV.md\"\n",
    "env_path.write_text('\\n'.join(env_lines))\n",
    "print(f\"Environment summary saved to {env_path}\")\n",
    "\n",
    "print(\"\\n=== TP Complete ===\")\n",
    "print(f\"Outputs saved to: {OUTPUTS_DIR}\")\n",
    "print(f\"Proof files saved to: {PROOF_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bda-env)",
   "language": "python",
   "name": "bda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
